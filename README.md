# Vision-Transformer-Image-Classification
Vision-Transformer-Image-Classification. A vision transformer (ViT) is a deep learning model used for image classification tasks. Unlike traditional convolutional neural network (CNN) models that use convolutional layers for feature extraction, ViT replaces convolutions with self-attention mechanisms, allowing the model to learn global image features without the need for spatial inductive biases. ViT uses a transformer architecture, which was originally designed for natural language processing tasks, to process the image data. The model first divides the image into patches and flattens them to create a sequence of vectors, which are then processed by multiple transformer layers. ViT has achieved state-of-the-art performance on several benchmark datasets and has the ability to generalize well to new images.

![ViT-diagram](https://github.com/enockkays/Vision-Transformer-Image-Classification/assets/32764779/78c53172-bb26-4a69-b8cc-a1dd6b9cd223)
